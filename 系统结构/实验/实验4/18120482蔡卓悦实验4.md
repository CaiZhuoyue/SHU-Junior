# 实验四 HPL安装和测试

<u>18120482 蔡卓悦</u>

## 实验4-1 HPL的安装和使用

#### 1.    实验目的

使学生掌握HPL的安装及运行。

 

#### 2.   主要软件包

1)   hpl-2.1.tar.gz

2)  GotoBLAS2-1.13.tar.gz   

3)  openmpi-1.6.5.tar.gz

 

#### 3.   实验内容

#### 3.1 HPL的安装

HPL安装步骤：

**(1)安装gotoblas**

a.   在`usr/local/mathlib/goto`下解压：

`$ tar -zxvf GotoBLAS2-1.13.tar.gz`

`$ cd GotoBLAS2`

`$ make  （TARGET=NEHALEM）`

**(2)安装openmpi（实验三内容）**

**(3)安装HPL**

a.   下载hpl-2.1.tar.gz (网址：http://www.netlib.org/benchmark/hpl/hpl-2.1.tar.gz)

b.   在用户目录下解压：

`$ tar –zxvf  hpl-2.1.tar.gz`

`$ cd hpl-2.1`

c.   根据机器的情况复制Makefile模板：

`$ cp setup/Make.Linux_PII_CBLAS Make.Linux`

`$ vi Make.Linux`

d.   如下根据具体情况修改 `Make.Linux`

```
ARCH = Linux 

TOPdir = /home/用户目录 / hpl-2.1

MPdir = /home/用户目录/openmpi安装目录/ openmpi-1.6.5

MPinc = $(MPdir)/include

MPlib = -L$(MPdir)/lib

LAdir = usr/local/mathlib/goto/GotoBLAS2的安装目录

LAlib = $(LAdir)/ libgoto2_nehalemp-r1.13.a

HPL_INCLUDES = -I$(INCdir) -I$(INCdir)/$(ARCH) -I$(LAinc) -I$(MPinc)

CC = /home/用户目录/openmpi的安装目录/bin/mpicc    

CCNOOPT = $(HPL_DEFS)

CCFLAGS = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops

LINKER = 同CC 

LINKFLAGS =同CCFLAGS
```

**编译**

a.   在HPL安装目录下运行

`$ make arch=Linux`



#### 3.2 HPL的运行

**HPL运行的实验步骤：**

**(1)进入 ～/安装hpl的目录/bin/Linux目录**

`$ cd /home/用户目录/hpl安装目录/bin/Linux`

**(2)准备节点文件**

`$ vi nodes`                                

**(3)修改 HPL.dat，设置运算规模和进程数等**

`$ vi HPL.dat`

**(4)运行**

`$ mpirun -np 4 –machinefile nodes xhpl`

 







**运行结果如下：**

![图片 1](/Users/caizhuoyue/Documents/Study/ComputerScience/2021Spring/系统结构/实验/实验4/截图/图片 1.png) 

![图片 2](/Users/caizhuoyue/Documents/Study/ComputerScience/2021Spring/系统结构/实验/实验4/截图/图片 2.png)

![图片 3](/Users/caizhuoyue/Documents/Study/ComputerScience/2021Spring/系统结构/实验/实验4/截图/图片 3.png)

![图片 4](/Users/caizhuoyue/Documents/Study/ComputerScience/2021Spring/系统结构/实验/实验4/截图/图片 4.png)



## 实验4-2 集群系统性能测试

### 1.    实验步骤

#### 1.1 计算计算机峰值速度

CPU主频：查看`/proc/cpuinfo`文件，将看见cpu的详细信息，其中 cpu MHz是主频值

网上查找资料计算峰值速度     

**理论浮点峰值**＝CPU主频×CPU每个时钟周期执行浮点运算的次数×系统中CPU数=2400MHz×4×1=9.6GMlops



#### 1.2 性能测试

使用gcc编译器的情况下测试，并将最佳测试结果填写下面表格

| 进程个数 | 4      | 5      | 6     | 7     |
| -------- | ------ | ------ | ----- | ----- |
| 峰值速度 | 38.4   | 48     | 57.6  | 67.2  |
| HPL测试  | 12.67  | 5.38   | 4.556 | 3.22  |
| 效率     | 32.99% | 11.21% | 7.91% | 4.79% |

 

| CPU个数 | N    | NB   | P    | Q    | Time | Gflops | 参与运算主机名       |
| ------- | ---- | ---- | ---- | ---- | ---- | ------ | -------------------- |
| 1       | 2048 | 60   | 2    | 2    | 0.42 | 13.45  | Master               |
| 2       | 2048 | 80   | 2    | 2    | 0.65 | 9.3    | Master/slave1        |
| 3       | 2048 | 80   | 2    | 2    | 1.65 | 3.5    | Master/slave1/slave2 |

 

#### 1.3 完成上述测试后比较和分析上面的测试结果，特别是如何能够得到高的性能测试值

​    通过实验结果，我们可以看到矩阵的分块大小对计算有影响。并且这个影响并不是线性的，当矩阵分块过大，那么会造成负载不均衡问题；而分块过小，会需要很大的通信开销，影响系统性能。但是可以看出会有一个最佳的矩阵分块大小存在，既能保证负载均衡，也能控制通信开销。

​    另外还能看出当数组的规模越大，性能就会越好。但是同样的到达一定阶段就会变差，这是由于内存的大小制约了系统性能，数组增大到超过内存的容量时，使用磁盘交换区的性能要远远低于内存的性能。

 

## 感想和心得

​    此次实验我们实验了使用HPL、openmpi、GotoBLAS软件，并且在实际实验中通过调整参数来提高性能表示值，虽然实验过程中出现了很多的问题，不过还是慢慢推进中解决了。老师给出的教程与实际环境有一定的出入，导致实验出现了一些意外，不过在同学的帮助下还是成功的完成了该实验。

​	

系统结构这门课在实验四结束之后就差不多正式的结束了。非常感谢雷老师上课时给我们的精彩的讲解，让我把前两年学到的计算机组成原理、操作系统、计算机网络等知识都串成了一个体系。同时我也从班级同学的小组演讲中，从他们的汇报中我学会了很多：比如编译器的优化到底是优化了什么，docker的功能和使用方式，hadoop的功能等等...



##### OpenMP、MPI和OpenMPI的区别

​    在最开始上这门课的时候，我对于这三者根本不了解，甚至很多时候会把它们记成同一个东西。后来在雷老师课上听老师讲课和课下自己完成系统结构的实验时，我发现这三者其实是有很多区别的。所以在这里我想总结一下这几个概念的区别，算是我做了这么多次实验的一个小收获。

​	它们的全称和作用分别是：

- **OpenMP：**英文全称是Open Multiprocessing，一种应用程序界面（API，即Application Program Interface），是一种并行的实现和方法，也可以认为是共享存储结构上的一种编程模型，可用于共享内存并行系统的多线程程序设计的一套指导性注释（Compiler Directive）。
- **MPI：**英文全称是Message Passing Interface，根据英文直接翻译可以得到“信息传递接口”，MP是独立于语言的通信协议(或标准)，是一个库而不是一门语言。MPI是在分布式内存）之间实现信息通讯的一种规范。MPI可以被fortran，c，c++等调用，允许静态任务调度，显示并行提供了良好的性能和移植性，用 MPI 编写的程序可直接在多核集群上运行。在集群系统中，集群的各节点之间可以采用 MPI 编程模型进行程序设计，每个节点都有自己的内存，可以对本地的指令和数据直接进行访问，各节点之间通过互联网络进行消息传递，这样设计具有很好的可移植性，完备的异步通信功能，较强的可扩展性等优点。
- **openMPI：**英文全称是open Message Passing Interface。openMPI是MPI的一种实现，一种库项目。

